{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekPWXciGW7K8"
      },
      "source": [
        "## Packages and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ4k5FbrWWTJ"
      },
      "outputs": [],
      "source": [
        "!pip install searchtweets\n",
        "!pip install GetOldTweets3\n",
        "!pip install snscrape\n",
        "!pip install PyDrive\n",
        "!pip install snorkel\n",
        "!pip install networkx\n",
        "!pip install DateTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mro_08qzYr-j"
      },
      "outputs": [],
      "source": [
        "# utilities, dataFrame, and API packages\n",
        "import json, os, requests, oauthlib, time, urllib, urllib.request, csv, zipfile, io, shutil, re, scipy, jinja2, urllib3\n",
        "import pandas as pd # package that allows us to make and manipulate dataFrames\n",
        "from pandas.io.json import json_normalize # package allows us to \"flatten\" json files, eg, extract nested data in json files\n",
        "from pprint import pprint # print output in a readable fashion\n",
        "import lxml\n",
        "import http.client\n",
        "from datetime import datetime\n",
        "from http.client import IncompleteRead\n",
        "from urllib.parse import urlparse, parse_qs, urlsplit, unquote # allows us to parse and work with URLs\n",
        "from DateTime import DateTime\n",
        "import re #regular expression package \n",
        "import csv # read/write csv package \n",
        "import string\n",
        "import time\n",
        "\n",
        "# statistical computing (including ML) and working with numbers\n",
        "import numpy as np \n",
        "import scipy.stats as stats # stastistics package \n",
        "import glob # NLP package\n",
        "\n",
        "# twitter API and data packages\n",
        "from searchtweets import ResultStream, gen_rule_payload, load_credentials \n",
        "import GetOldTweets3 as got\n",
        "import snscrape\n",
        "from snscrape import modules\n",
        "from snscrape.modules import twitter\n",
        "import tweepy\n",
        "from tweepy.streaming import StreamListener\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import Stream\n",
        "import requests\n",
        "\n",
        "# data visualization packages\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# Google Drive packages\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching For Tweets w/ Keywords and Phrases"
      ],
      "metadata": {
        "id": "P-9C5kzdkSRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sd94Cu9Z6Wn"
      },
      "outputs": [],
      "source": [
        "# Use snscrape to pull tweet urls containing key words and phases\n",
        "'''\n",
        "!snscrape --since [start date] twitter-search \"[keyword/phrase] until:[end date]\" > [save file path]\n",
        "''''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pulling Tweets and Reference Tweets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u7F4Rp-ckeXp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB3UGrk43gsH"
      },
      "source": [
        "## Function Library\n",
        "\n",
        "* replaceBrackets(tweetIDs)\n",
        "  * tweetIDs is formated as a 2D list, where each element is a list with one string eleemnt holding an ID \n",
        "    * ie. [['123'], ['456'], ...]\n",
        "  * Returns a list of tweet IDs as strings \n",
        "    * ie. ['123', '456', ...]\n",
        "* make100(tweetList, n)\n",
        "  * tweetList is formatted as a list of tweet IDs as a string\n",
        "    * ie. ['123', '456', ...]\n",
        "  * Returns the nth list of 100 tweet IDs\n",
        "* convertFormat(tweets)\n",
        "  * tweets is formatted as a list of tweet IDs as a string\n",
        "    * ie. ['123', '456', ...]\n",
        "  * Returns the provided tweet IDs as a concatenated string separated by commas\n",
        "    * ie. \"123,456,...\"\n",
        "* pull_data(tweet_ids, n)\n",
        "  * tweet_ids is formatted as a list of tweet IDs as a string\n",
        "    * ie. ['123', '456', ...]\n",
        "  * n = level in tree (ie. 0 is first scrape, 1 is second, 2 is third, etc)\n",
        "  * Requests tweet data from Twitter's API for given list of tweet IDs in groups of 100, organizes it, and stores it as a CSV file\n",
        "  * Depends on: make100(), convertFormat()\n",
        "* merge_files(wdir, filename_format, save_path)\n",
        "  * Merges all csv files following specified file_format in wdir into one and saves to save_path\n",
        "* parse_dict(s)\n",
        "  * s is a string containing a list containing a dictionary\n",
        "    * ie. \"[{'key': \"data\"}]\"\n",
        "  * Returns the dictionary contained within the string\n",
        "* label_toplevel(file_path)\n",
        "  * Adds a top_level column to the dataframe and tags the toplevel tweets\n",
        "* convert_time_format(file_path, save_path)\n",
        "  * Takes data in created_at column and formats it for easy use\n",
        "  * Note: Converts a given csv file to a pandas dataframe and saves as a csv file\n",
        "* extract_above_level(file_path)\n",
        "  * Extracts tweet id of referenced tweet from dictionary returned by parse_dict()\n",
        "  * Stores and returns ids in a list formatted for pull_data()\n",
        "  * Depends on parse_dict()\n",
        "* not_all_toplevel(file_path)\n",
        "  * Returns true if given tweets includes non-toplevel tweets, and false otherwise\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEbw28C_fQGp"
      },
      "outputs": [],
      "source": [
        "# Written by Proteeti Sinha\n",
        "\n",
        "# tweetIDs is formated as a 2D list, where each element is a list with one string eleemnt holding an ID\n",
        "# ie. [['123'], ['456'], ...]\n",
        "\n",
        "# Returns a list of tweet IDs as strings\n",
        "# ie. ['123', '456', ...]\n",
        "\n",
        "def replaceBrackets(tweetIDs): \n",
        "    result = [] \n",
        "    for tweetID in tweetIDs:\n",
        "        tweetID = str(tweetID)\n",
        "        tweetID = tweetID.replace(\"['\", \"\")\n",
        "        tweetID = tweetID.replace(\"']\", \"\")\n",
        "        result.append(tweetID)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIEUbBqnfYxP"
      },
      "outputs": [],
      "source": [
        "# Written by Proteeti Sinha\n",
        "\n",
        "# tweetList is formatted as a list of tweet IDs as a string\n",
        "# ie. ['123', '456', ...]\n",
        "\n",
        "# Returns the nth list of 100 tweet IDs\n",
        "\n",
        "def make100(tweetList, n):\n",
        "    result = []\n",
        "    current = n * 100\n",
        "    listLen = len(tweetList)\n",
        "    if (listLen - current) < 100:\n",
        "        for i in range(current, listLen):\n",
        "            result.append(tweetList[i])\n",
        "        return result\n",
        "    else:\n",
        "        for i in range(current, current + 100):\n",
        "            result.append(tweetList[i])\n",
        "        return result \n",
        "    print(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQXoTwVywyW3"
      },
      "outputs": [],
      "source": [
        "# Written by Proteeti Sinha\n",
        "\n",
        "# tweets is formatted as a list of tweet IDs as a string\n",
        "# ie. ['123', '456', ...]\n",
        "\n",
        "# Returns the provided tweet IDs as a concatenated string separated by commas\n",
        "# ie. \"123,456,...\"\n",
        "\n",
        "def convertFormat(tweets):\n",
        "    result = \"\"\n",
        "    for tweet in range(len(tweets)-1):\n",
        "        # print(tweets[tweet])\n",
        "        numConvert = str(int(tweets[tweet]))\n",
        "        result = result + numConvert + \",\"\n",
        "    result = result + str(int(tweets[-1]))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue3DSsgkBsb2"
      },
      "outputs": [],
      "source": [
        "# tweet_ids is formatted as a list of tweet IDs as a string\n",
        "# ie. ['123', '456', ...]\n",
        "\n",
        "# n = level in tree (ie. 0 is first scrape, 1 is second, 2 is third, etc)\n",
        "\n",
        "# Requests tweet data from Twitter's API for given list of tweet IDs in groups of 100, organizes it, and stores it as a CSV file\n",
        "# Depends on: make100(), convertFormat()\n",
        "\n",
        "def pull_data(tweet_ids, n):\n",
        "\n",
        "  # TODO: Change authorization credentials\n",
        "  headers = {\n",
        "      'Authorization': ''\n",
        "  }\n",
        "\n",
        "  # Calculates the number of groups of a hundred ids in tweet_ids\n",
        "  x = len(tweet_ids) / 100\n",
        "  num_of_hundreds = int(x)\n",
        "  if x > num_of_hundreds: num_of_hundreds += 1\n",
        "\n",
        "  # Debugging printlines\n",
        "  print(\"Length of tweet_ids: \", len(tweet_ids))\n",
        "  print(\"Number of groups of a hundred:\", num_of_hundreds)\n",
        "\n",
        "  # For loop automates process of pulling data for groups of 100\n",
        "  for i in range(num_of_hundreds):\n",
        "    print(\"Hundred #: \", i)\n",
        "    hundo = make100(tweet_ids, i)\n",
        "\n",
        "    params = (\n",
        "      ('ids', convertFormat(hundo)),\n",
        "      ('expansions', 'author_id,attachments.poll_ids,attachments.media_keys,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id'),\n",
        "      ('place.fields','contained_within,country,country_code,full_name,geo,id,name,place_type'),\n",
        "      ('poll.fields','duration_minutes,end_datetime,id,options,voting_status'),\n",
        "      ('tweet.fields', 'public_metrics,attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld'),\n",
        "      ('media.fields', 'duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics'),\n",
        "      ('user.fields','created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'),\n",
        "    )\n",
        "\n",
        "    # API Request\n",
        "    response = requests.get('https://api.twitter.com/2/tweets', headers=headers, params=params)\n",
        "    time.sleep(5)\n",
        "\n",
        "    c = json.loads(response.text)\n",
        "    cc=c['data']\n",
        "    c_norm=pd.json_normalize(cc)\n",
        "\n",
        "    # Creates an empty dataframe that is then appended on to\n",
        "    # User has control over column inclusion and order\n",
        "    extractedDF = pd.DataFrame(c_norm)\n",
        "\n",
        "    tweetDF = pd.DataFrame(columns = ['id', 'created_at', 'text', \n",
        "                     'conversation_id', 'author_id', 'referenced_tweets', \n",
        "                     'in_reply_to_user_id', 'lang', 'possibly_sensitive', \n",
        "                     'reply_settings', 'source', 'context_annotations', \n",
        "                     'public_metrics.like_count', 'public_metrics.retweet_count', 'public_metrics.reply_count',\n",
        "                     'public_metrics.quote_count', 'attachments.media_keys', 'attachments.poll_ids',\n",
        "                     'entities.hashtags', 'entities.urls', 'entities.mentions',\n",
        "                     'entities.annotations', 'geo.place_id'])\n",
        "    \n",
        "    tweetDF = tweetDF.append(extractedDF)\n",
        "\n",
        "    tweetDF.to_csv(f'/content/drive/MyDrive/Research/CHIMPS/(Data) Portrait AI/Iteration 3/PAI Tweets/100 Groups/PAI_{n}.{i}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OieSNgYbZOjX"
      },
      "outputs": [],
      "source": [
        "# Merges all csv files following specified file_format in wdir into one and saves to save_path\n",
        "\n",
        "def merge_files(wdir, file_format, save_path):\n",
        "  # https://blog.softhints.com/how-to-merge-multiple-csv-files-with-python/#step1importmodulesandsettheworkingdirectory\n",
        "  all_files = glob.glob(os.path.join(wdir, file_format))\n",
        "  df_from_each_file = (pd.read_csv(f, sep=',') for f in all_files)\n",
        "  df_merged = pd.concat(df_from_each_file, ignore_index=True)\n",
        "  df_merged.to_csv(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR-k7wS-jlPw"
      },
      "outputs": [],
      "source": [
        "# s is a string containing a list containing a dictionary\n",
        "# ie. \"[{'key': \"data\"}]\"\n",
        "\n",
        "# Returns the dictionary contained within the string\n",
        "\n",
        "def parse_dict(s): \n",
        "    # https://www.w3schools.com/python/ref_string_replace.asp\n",
        "    one = s.replace(\"[\", \"\")\n",
        "    two = one.replace(\"]\", \"\")\n",
        "\n",
        "    # https://www.geeksforgeeks.org/python-program-to-create-a-dictionary-from-a-string/\n",
        "    d = eval(two)\n",
        "    \n",
        "    return(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBG6ZtMDEF2q"
      },
      "outputs": [],
      "source": [
        "# Adds a top_level column to the dataframe and tags the toplevel tweets\n",
        "\n",
        "def label_toplevel(file_path):\n",
        "  df = pd.read_csv(file_path)\n",
        "\n",
        "  replies_df = pd.get_dummies(df, dummy_na=True, columns=pd.Series('in_reply_to_user_id'))\n",
        "  top_level = replies_df['in_reply_to_user_id_nan']\n",
        "  df['top_level'] = top_level\n",
        "\n",
        "  df.to_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhIgyM0e72nq"
      },
      "outputs": [],
      "source": [
        "# Takes data in created_at column and formats it for easy use\n",
        "# Note: Converts a given csv file to a pandas dataframe and saves as a csv file\n",
        "\n",
        "def convert_time_format(file_path, save_path):\n",
        "  df = pd.read_csv(file_path)\n",
        "  num_rows = len(df)\n",
        "\n",
        "  created_at_col_idx = df.columns.get_loc('created_at')\n",
        "\n",
        "  for i in range(num_rows):\n",
        "    created_datetime = df.iloc[i, created_at_col_idx]\n",
        "    # https://pypi.org/project/DateTime/\n",
        "    x = DateTime(created_datetime)\n",
        "    df.iloc[i, created_at_col_idx] = x.ISO()\n",
        "\n",
        "  df.to_csv(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ola_gDPm8QrN"
      },
      "outputs": [],
      "source": [
        "# Extracts tweet id of referenced tweet from dictionary returned by parse_dict()\n",
        "# Stores and returns ids in a list formatted for pull_data()\n",
        "\n",
        "def extract_above_level(file_path):\n",
        "  df = pd.read_csv(file_path)\n",
        "  num_rows = len(df)\n",
        "\n",
        "  ref_tweets_col_idx = df.columns.get_loc('referenced_tweets')\n",
        "\n",
        "  to_extract_ids = []\n",
        "\n",
        "  for i in range(num_rows):\n",
        "    # print(i)\n",
        "    # https://towardsdatascience.com/how-to-use-loc-and-iloc-for-selecting-data-in-pandas-bd09cb4c3d79\n",
        "    ref_tweet_info = df.iloc[i, ref_tweets_col_idx]\n",
        "    if type(ref_tweet_info) == str:\n",
        "      info_dict = parse_dict(ref_tweet_info)\n",
        "      if type(info_dict) != dict: continue\n",
        "      id = info_dict['id']\n",
        "      to_extract_ids.append(id)\n",
        "  \n",
        "  return to_extract_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d2f7ATqFpA-"
      },
      "outputs": [],
      "source": [
        "# Returns true if given tweets includes non-toplevel tweets, and false otherwise\n",
        "\n",
        "def not_all_toplevel(file_path):\n",
        "  df = pd.read_csv(file_path)\n",
        "  num_rows = len(df)\n",
        "\n",
        "  toplevel_col_idx = df.columns.get_loc('top_level')\n",
        "\n",
        "  for i in range(num_rows):\n",
        "    is_toplevel = df.iloc[i, toplevel_col_idx]\n",
        "    if is_toplevel == 0: return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh-wO4wFEtc3"
      },
      "source": [
        "Filename Library\n",
        "\n",
        "\"PAI_a.b\" --> files storing raw data for groups of 100 (a = level #, b = file #)\n",
        "\n",
        "\"PAI_merged_a\" --> files storing merged\n",
        "\n",
        "\"PAI_all_a\" --> files storing engineered data for level a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdO73Ahyd-x5"
      },
      "source": [
        "## Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yDlb5el9W6J"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  # TODO: Add scraped tweet urls path \n",
        "  tweet_urls = \"\"\n",
        "\n",
        "  # TODO: Add path of folder where all scraped data will be saved\n",
        "  working_directory = \"\"\n",
        "\n",
        "  # TODO: Add a file name format\n",
        "  # i.e. \"[data set name]_0.*.csv\"\n",
        "  # file names will update to be \"[data set name]_[level].[nth 100].csv\"\n",
        "  file_format = \"\"\n",
        "\n",
        "  # TODO: Add a merged file path and name (where the groups of 100 from one level are merged together)\n",
        "  # i.e. \"[data set name]_merged_0.csv\"\n",
        "  # file names will update to be \"[data set name]_merged_[level].csv\"\n",
        "  merged_file_path = \"\" \n",
        "\n",
        "  # TODO: Add a save file path and name (where a copy of the merged files are saved)\n",
        "  # i.e. \"[data set name]_all_0.csv\"\n",
        "  # file names will update to be \"[data set name]_all_[level].csv\"\n",
        "  save_path = \"\"\n",
        "\n",
        "  # CSV --> pandas dataframe\n",
        "  tweets = pd.read_csv(tweet_urls, header=None)\n",
        "\n",
        "  # Written Proteeti Sinha\n",
        "  # Parsing out tweet ids from urls\n",
        "  urlz=tweets[0]\n",
        "  tweetIDs = []\n",
        "  for url in urlz:\n",
        "    tweetRegex = re.findall(r'\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d+', url)\n",
        "    tweetIDs.append(tweetRegex)\n",
        "\n",
        "  # Configuring 2D list of IDs as strings to a list of IDs as strings\n",
        "  tweetListOverall = replaceBrackets(tweetIDs)\n",
        "\n",
        "  print(\"Level: 0\")\n",
        "\n",
        "  # Pulling data from urls give above\n",
        "  pull_data(tweetListOverall, 0)\n",
        "\n",
        "  # Merging all the resulting files and saving it into one file\n",
        "  merge_files(working_directory, file_format, merged_file_path)\n",
        "\n",
        "  # Extracts tweet ids from level above\n",
        "  above_level_tweet_ids = extract_above_level(merged_file_path) # updates as loop runs\n",
        "  # Labels the toplevel tweets\n",
        "  label_toplevel(merged_file_path)\n",
        "  # Converts the time formatting and saves it into another file\n",
        "  convert_time_format(merged_file_path, save_path)\n",
        "\n",
        "  print(\"Level: 0 finish\")\n",
        "  print()\n",
        "\n",
        "  # Runs after first base data set of urls are done pulling\n",
        "  # Collects and pulls data for higher level tweets, until all toplevel tweets are found and included\n",
        "  count = 1 # level 1\n",
        "\n",
        "  while not_all_toplevel(save_path) and len(above_level_tweet_ids) > 0:\n",
        "    print(\"Level: \", count)\n",
        "    pull_data(above_level_tweet_ids, count)\n",
        "\n",
        "    # TODO: Add a file name format\n",
        "    # i.e. \"[data set name]_{count}.*.csv\"\n",
        "    # file names will update to be \"[data set name]_[level].[nth 100].csv\"\n",
        "    file_format = f\"\"\n",
        "\n",
        "    # TODO: Add a merged file path and name (where the nth data from one level are merged together)\n",
        "    # i.e. \"[data set name]_merged_{count}.csv\"\n",
        "    # file names will update to be \"[data set name]_merged_[level].csv\"\n",
        "    merged_file_path = f\"\"\n",
        "\n",
        "    # TODO: Add a save file path and name (where a copy of the merged files are saved)\n",
        "    # i.e. \"[data set name]_all_{count}.csv\"\n",
        "    # file names will update to be \"[data set name]_all_[level].csv\"\n",
        "    save_path = f\"\"\n",
        "\n",
        "    merge_files(working_directory, file_format, merged_file_path)\n",
        "\n",
        "    above_level_tweet_ids = extract_above_level(merged_file_path)\n",
        "    label_toplevel(merged_file_path)\n",
        "    convert_time_format(merged_file_path, save_path)\n",
        "\n",
        "    print(\"Level: \", count, \"finish\")\n",
        "    print()\n",
        "    count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEgZXcGcPO8X"
      },
      "outputs": [],
      "source": [
        "# Run after updating paths in main() and pull_data()\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge all the generated files into one"
      ],
      "metadata": {
        "id": "FEskpiB3p9MN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPxyKwAam9wG"
      },
      "outputs": [],
      "source": [
        "# Merge all the final data files and remove unnecessary columns\n",
        "\n",
        "# TODO: Add path to folder saving files with \"[data set name]_all_[level].csv\"\n",
        "wdir = \"\"\n",
        "# TODO: Add filename format \"[data set name]_all_*.csv\"\n",
        "filename_format = \"\"\n",
        "# TODO: Add save file name and \n",
        "save_path = \"\"\n",
        "\n",
        "merge_files(wdir, filename_format, save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1jPUDhftfhLe"
      ],
      "name": "twitter_scrape_github.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}